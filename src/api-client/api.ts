/* tslint:disable */
/**
 * Firecrawl API
 * API for interacting with Firecrawl services to perform web scraping and crawling tasks.
 *
 * The version of the OpenAPI document: v1
 * Contact: support@firecrawl.dev
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */

import type { Configuration } from './configuration.js';
import type { AxiosPromise, AxiosInstance, RawAxiosRequestConfig } from 'axios';
import globalAxios from 'axios';
// Some imports not used depending on template conditions
// @ts-ignore
import {
  DUMMY_BASE_URL,
  assertParamExists,
  setApiKeyToObject,
  setBasicAuthToObject,
  setBearerAuthToObject,
  setOAuthToObject,
  setSearchParams,
  serializeDataIfNeeded,
  toPathString,
  createRequestFunction,
} from './common.js';
import type { RequestArgs } from './base.js';
// @ts-ignore
import {
  BASE_PATH,
  COLLECTION_FORMATS,
  BaseAPI,
  RequiredError,
  operationServerMap,
} from './base.js';

/**
 *
 * @export
 * @interface BatchScrapeResponseObj
 */
export interface BatchScrapeResponseObj {
  /**
   *
   * @type {boolean}
   * @memberof BatchScrapeResponseObj
   */
  success?: boolean;
  /**
   *
   * @type {string}
   * @memberof BatchScrapeResponseObj
   */
  id?: string;
  /**
   *
   * @type {string}
   * @memberof BatchScrapeResponseObj
   */
  url?: string;
  /**
   * If ignoreInvalidURLs is true, this is an array containing the invalid URLs that were specified in the request. If there were no invalid URLs, this will be an empty array. If ignoreInvalidURLs is false, this field will be undefined.
   * @type {Array<string>}
   * @memberof BatchScrapeResponseObj
   */
  invalidURLs?: Array<string> | null;
}
/**
 *
 * @export
 * @interface BatchScrapeStatusResponseObj
 */
export interface BatchScrapeStatusResponseObj {
  /**
   * The current status of the batch scrape. Can be `scraping`, `completed`, or `failed`.
   * @type {string}
   * @memberof BatchScrapeStatusResponseObj
   */
  status?: string;
  /**
   * The total number of pages that were attempted to be scraped.
   * @type {number}
   * @memberof BatchScrapeStatusResponseObj
   */
  total?: number;
  /**
   * The number of pages that have been successfully scraped.
   * @type {number}
   * @memberof BatchScrapeStatusResponseObj
   */
  completed?: number;
  /**
   * The number of credits used for the batch scrape.
   * @type {number}
   * @memberof BatchScrapeStatusResponseObj
   */
  creditsUsed?: number;
  /**
   * The date and time when the batch scrape will expire.
   * @type {string}
   * @memberof BatchScrapeStatusResponseObj
   */
  expiresAt?: string;
  /**
   * The URL to retrieve the next 10MB of data. Returned if the batch scrape is not completed or if the response is larger than 10MB.
   * @type {string}
   * @memberof BatchScrapeStatusResponseObj
   */
  next?: string | null;
  /**
   * The data of the batch scrape.
   * @type {Array<CrawlStatusResponseObjDataInner>}
   * @memberof BatchScrapeStatusResponseObj
   */
  data?: Array<CrawlStatusResponseObjDataInner>;
}
/**
 *
 * @export
 * @interface CancelBatchCrawl200Response
 */
export interface CancelBatchCrawl200Response {
  /**
   *
   * @type {string}
   * @memberof CancelBatchCrawl200Response
   */
  status?: CancelBatchCrawl200ResponseStatusEnum;
}

export const CancelBatchCrawl200ResponseStatusEnum = {
  Cancelled: 'cancelled',
} as const;

export type CancelBatchCrawl200ResponseStatusEnum =
  (typeof CancelBatchCrawl200ResponseStatusEnum)[keyof typeof CancelBatchCrawl200ResponseStatusEnum];

/**
 *
 * @export
 * @interface CancelBatchCrawl404Response
 */
export interface CancelBatchCrawl404Response {
  /**
   *
   * @type {string}
   * @memberof CancelBatchCrawl404Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface Click
 */
export interface Click {
  /**
   * Click on an element
   * @type {string}
   * @memberof Click
   */
  type: ClickTypeEnum;
  /**
   * Query selector to find the element by
   * @type {string}
   * @memberof Click
   */
  selector: string;
}

export const ClickTypeEnum = {
  Click: 'click',
} as const;

export type ClickTypeEnum = (typeof ClickTypeEnum)[keyof typeof ClickTypeEnum];

/**
 *
 * @export
 * @interface CrawlResponse
 */
export interface CrawlResponse {
  /**
   *
   * @type {boolean}
   * @memberof CrawlResponse
   */
  success?: boolean;
  /**
   *
   * @type {string}
   * @memberof CrawlResponse
   */
  id?: string;
  /**
   *
   * @type {string}
   * @memberof CrawlResponse
   */
  url?: string;
}
/**
 *
 * @export
 * @interface CrawlStatusResponseObj
 */
export interface CrawlStatusResponseObj {
  /**
   * The current status of the crawl. Can be `scraping`, `completed`, or `failed`.
   * @type {string}
   * @memberof CrawlStatusResponseObj
   */
  status?: string;
  /**
   * The total number of pages that were attempted to be crawled.
   * @type {number}
   * @memberof CrawlStatusResponseObj
   */
  total?: number;
  /**
   * The number of pages that have been successfully crawled.
   * @type {number}
   * @memberof CrawlStatusResponseObj
   */
  completed?: number;
  /**
   * The number of credits used for the crawl.
   * @type {number}
   * @memberof CrawlStatusResponseObj
   */
  creditsUsed?: number;
  /**
   * The date and time when the crawl will expire.
   * @type {string}
   * @memberof CrawlStatusResponseObj
   */
  expiresAt?: string;
  /**
   * The URL to retrieve the next 10MB of data. Returned if the crawl is not completed or if the response is larger than 10MB.
   * @type {string}
   * @memberof CrawlStatusResponseObj
   */
  next?: string | null;
  /**
   * The data of the crawl.
   * @type {Array<CrawlStatusResponseObjDataInner>}
   * @memberof CrawlStatusResponseObj
   */
  data?: Array<CrawlStatusResponseObjDataInner>;
}
/**
 *
 * @export
 * @interface CrawlStatusResponseObjDataInner
 */
export interface CrawlStatusResponseObjDataInner {
  /**
   *
   * @type {string}
   * @memberof CrawlStatusResponseObjDataInner
   */
  markdown?: string;
  /**
   * HTML version of the content on page if `includeHtml`  is true
   * @type {string}
   * @memberof CrawlStatusResponseObjDataInner
   */
  html?: string | null;
  /**
   * Raw HTML content of the page if `includeRawHtml`  is true
   * @type {string}
   * @memberof CrawlStatusResponseObjDataInner
   */
  rawHtml?: string | null;
  /**
   * List of links on the page if `includeLinks` is true
   * @type {Array<string>}
   * @memberof CrawlStatusResponseObjDataInner
   */
  links?: Array<string>;
  /**
   * Screenshot of the page if `includeScreenshot` is true
   * @type {string}
   * @memberof CrawlStatusResponseObjDataInner
   */
  screenshot?: string | null;
  /**
   *
   * @type {ScrapeResponseDataMetadata}
   * @memberof CrawlStatusResponseObjDataInner
   */
  metadata?: ScrapeResponseDataMetadata;
}
/**
 *
 * @export
 * @interface CrawlUrlsRequest
 */
export interface CrawlUrlsRequest {
  /**
   * The base URL to start crawling from
   * @type {string}
   * @memberof CrawlUrlsRequest
   */
  url: string;
  /**
   * Specifies URL patterns to exclude from the crawl by comparing website paths against the provided regex patterns. For example, if you set \"excludePaths\": [\"blog/_*\"] for the base URL firecrawl.dev, any results matching that pattern will be excluded, such as https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap.
   * @type {Array<string>}
   * @memberof CrawlUrlsRequest
   */
  excludePaths?: Array<string>;
  /**
   * Specifies URL patterns to include in the crawl by comparing website paths against the provided regex patterns. Only the paths that match the specified patterns will be included in the response. For example, if you set \"includePaths\": [\"blog/_*\"] for the base URL firecrawl.dev, only results matching that pattern will be included, such as https://www.firecrawl.dev/blog/firecrawl-launch-week-1-recap.
   * @type {Array<string>}
   * @memberof CrawlUrlsRequest
   */
  includePaths?: Array<string>;
  /**
   * Maximum depth to crawl relative to the entered URL.
   * @type {number}
   * @memberof CrawlUrlsRequest
   */
  maxDepth?: number;
  /**
   * Maximum depth to crawl based on discovery order.
   * @type {number}
   * @memberof CrawlUrlsRequest
   */
  maxDiscoveryDepth?: number;
  /**
   * Do not re-scrape the same path with different (or none) query parameters
   * @type {boolean}
   * @memberof CrawlUrlsRequest
   */
  ignoreQueryParameters?: boolean;
  /**
   * Ignore the website sitemap when crawling
   * @type {boolean}
   * @memberof CrawlUrlsRequest
   */
  ignoreSitemap?: boolean;
  /**
   * Maximum number of pages to crawl. Default limit is 10000.
   * @type {number}
   * @memberof CrawlUrlsRequest
   */
  limit?: number;
  /**
   * Enables the crawler to navigate from a specific URL to previously linked pages.
   * @type {boolean}
   * @memberof CrawlUrlsRequest
   */
  allowBackwardLinks?: boolean;
  /**
   * Allows the crawler to follow links to external websites.
   * @type {boolean}
   * @memberof CrawlUrlsRequest
   */
  allowExternalLinks?: boolean;
  /**
   * Delay in seconds between scrapes. This helps respect website rate limits.
   * @type {number}
   * @memberof CrawlUrlsRequest
   */
  delay?: number;
  /**
   *
   * @type {CrawlUrlsRequestWebhook}
   * @memberof CrawlUrlsRequest
   */
  webhook?: CrawlUrlsRequestWebhook;
  /**
   *
   * @type {CrawlUrlsRequestScrapeOptions}
   * @memberof CrawlUrlsRequest
   */
  scrapeOptions?: CrawlUrlsRequestScrapeOptions;
}
/**
 *
 * @export
 * @interface CrawlUrlsRequestScrapeOptions
 */
export interface CrawlUrlsRequestScrapeOptions {
  /**
   * Formats to include in the output.
   * @type {Array<string>}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  formats?: Array<CrawlUrlsRequestScrapeOptionsFormatsEnum>;
  /**
   * Headers to send with the request. Can be used to send cookies, user-agent, etc.
   * @type {object}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  headers?: object;
  /**
   * Tags to include in the output.
   * @type {Array<string>}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  includeTags?: Array<string>;
  /**
   * Tags to exclude from the output.
   * @type {Array<string>}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  excludeTags?: Array<string>;
  /**
   * Only return the main content of the page excluding headers, navs, footers, etc.
   * @type {boolean}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  onlyMainContent?: boolean;
  /**
   * Remove base64 encoded images from the output
   * @type {boolean}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  removeBase64Images?: boolean;
  /**
   * Set to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.
   * @type {boolean}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  mobile?: boolean;
  /**
   * Wait x amount of milliseconds for the page to load to fetch content
   * @type {number}
   * @memberof CrawlUrlsRequestScrapeOptions
   */
  waitFor?: number;
}

export const CrawlUrlsRequestScrapeOptionsFormatsEnum = {
  Markdown: 'markdown',
  Html: 'html',
  RawHtml: 'rawHtml',
  Links: 'links',
  Screenshot: 'screenshot',
} as const;

export type CrawlUrlsRequestScrapeOptionsFormatsEnum =
  (typeof CrawlUrlsRequestScrapeOptionsFormatsEnum)[keyof typeof CrawlUrlsRequestScrapeOptionsFormatsEnum];

/**
 * @type CrawlUrlsRequestWebhook
 * @export
 */
export type CrawlUrlsRequestWebhook = CrawlUrlsRequestWebhookOneOf | string;

/**
 * A complex webhook specification object.
 * @export
 * @interface CrawlUrlsRequestWebhookOneOf
 */
export interface CrawlUrlsRequestWebhookOneOf {
  /**
   * The URL to send the webhook to. This will trigger for crawl started (crawl.started), every page crawled (crawl.page) and when the crawl is completed (crawl.completed or crawl.failed). The response will be the same as the `/scrape` endpoint.
   * @type {string}
   * @memberof CrawlUrlsRequestWebhookOneOf
   */
  url: string;
  /**
   * Headers to send to the webhook URL.
   * @type {{ [key: string]: string; }}
   * @memberof CrawlUrlsRequestWebhookOneOf
   */
  headers?: { [key: string]: string };
  /**
   * Custom metadata that will be included in all webhook payloads for this crawl
   * @type {{ [key: string]: any; }}
   * @memberof CrawlUrlsRequestWebhookOneOf
   */
  metadata?: { [key: string]: any };
  /**
   * Type of events that should be sent to the webhook URL. (default: all)
   * @type {Array<CrawlUrlsRequestWebhookOneOfEventsEnum>}
   * @memberof CrawlUrlsRequestWebhookOneOf
   */
  events?: Array<CrawlUrlsRequestWebhookOneOfEventsEnum>;
}

export const CrawlUrlsRequestWebhookOneOfEventsEnum = {
  Completed: 'completed',
  Page: 'page',
  Failed: 'failed',
  Started: 'started',
} as const;

export type CrawlUrlsRequestWebhookOneOfEventsEnum =
  (typeof CrawlUrlsRequestWebhookOneOfEventsEnum)[keyof typeof CrawlUrlsRequestWebhookOneOfEventsEnum];
/**
 *
 * @export
 * @interface ExecuteJavaScript
 */
export interface ExecuteJavaScript {
  /**
   * Execute JavaScript code on the page
   * @type {string}
   * @memberof ExecuteJavaScript
   */
  type: ExecuteJavaScriptTypeEnum;
  /**
   * JavaScript code to execute
   * @type {string}
   * @memberof ExecuteJavaScript
   */
  script: string;
}

export const ExecuteJavaScriptTypeEnum = {
  ExecuteJavascript: 'executeJavascript',
} as const;

export type ExecuteJavaScriptTypeEnum =
  (typeof ExecuteJavaScriptTypeEnum)[keyof typeof ExecuteJavaScriptTypeEnum];

/**
 *
 * @export
 * @interface ExtractData400Response
 */
export interface ExtractData400Response {
  /**
   *
   * @type {string}
   * @memberof ExtractData400Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface ExtractDataRequest
 */
export interface ExtractDataRequest {
  /**
   *
   * @type {Array<string>}
   * @memberof ExtractDataRequest
   */
  urls?: Array<string>;
  /**
   * Prompt to guide the extraction process
   * @type {string}
   * @memberof ExtractDataRequest
   */
  prompt?: string;
  /**
   *
   * @type {ExtractDataRequestSchema}
   * @memberof ExtractDataRequest
   */
  schema?: ExtractDataRequestSchema;
  /**
   * When true, the extraction will use web search to find additional data
   * @type {boolean}
   * @memberof ExtractDataRequest
   */
  enableWebSearch?: boolean;
  /**
   * When true, sources used for extraction will be included in the response
   * @type {boolean}
   * @memberof ExtractDataRequest
   */
  showSources?: boolean;
}
/**
 * Schema to define the structure of the extracted data
 * @export
 * @interface ExtractDataRequestSchema
 */
export interface ExtractDataRequestSchema {
  /**
   * Description of property1
   * @type {string}
   * @memberof ExtractDataRequestSchema
   */
  property1: string;
  /**
   * Description of property2
   * @type {number}
   * @memberof ExtractDataRequestSchema
   */
  property2: number;
}
/**
 *
 * @export
 * @interface ExtractResponse
 */
export interface ExtractResponse {
  /**
   *
   * @type {boolean}
   * @memberof ExtractResponse
   */
  success?: boolean;
  /**
   *
   * @type {ExtractResponseData}
   * @memberof ExtractResponse
   */
  data?: ExtractResponseData;
}
/**
 *
 * @export
 * @interface ExtractResponseData
 */
export interface ExtractResponseData {
  /**
   *
   * @type {string}
   * @memberof ExtractResponseData
   */
  '&lt;property1&gt;'?: string;
  /**
   *
   * @type {number}
   * @memberof ExtractResponseData
   */
  '&lt;property2&gt;'?: number;
}
/**
 *
 * @export
 * @interface GetCreditUsage200Response
 */
export interface GetCreditUsage200Response {
  /**
   *
   * @type {boolean}
   * @memberof GetCreditUsage200Response
   */
  success?: boolean;
  /**
   *
   * @type {GetCreditUsage200ResponseData}
   * @memberof GetCreditUsage200Response
   */
  data?: GetCreditUsage200ResponseData;
}
/**
 *
 * @export
 * @interface GetCreditUsage200ResponseData
 */
export interface GetCreditUsage200ResponseData {
  /**
   * Number of credits remaining for the team
   * @type {number}
   * @memberof GetCreditUsage200ResponseData
   */
  remaining_credits?: number;
}
/**
 *
 * @export
 * @interface GetCreditUsage404Response
 */
export interface GetCreditUsage404Response {
  /**
   *
   * @type {boolean}
   * @memberof GetCreditUsage404Response
   */
  success?: boolean;
  /**
   *
   * @type {string}
   * @memberof GetCreditUsage404Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface GetCreditUsage500Response
 */
export interface GetCreditUsage500Response {
  /**
   *
   * @type {boolean}
   * @memberof GetCreditUsage500Response
   */
  success?: boolean;
  /**
   *
   * @type {string}
   * @memberof GetCreditUsage500Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface MapResponse
 */
export interface MapResponse {
  /**
   *
   * @type {boolean}
   * @memberof MapResponse
   */
  success?: boolean;
  /**
   *
   * @type {Array<string>}
   * @memberof MapResponse
   */
  links?: Array<string>;
}
/**
 *
 * @export
 * @interface MapUrlsRequest
 */
export interface MapUrlsRequest {
  /**
   * The base URL to start crawling from
   * @type {string}
   * @memberof MapUrlsRequest
   */
  url: string;
  /**
   * Search query to use for mapping. During the Alpha phase, the \'smart\' part of the search functionality is limited to 1000 search results. However, if map finds more results, there is no limit applied.
   * @type {string}
   * @memberof MapUrlsRequest
   */
  search?: string;
  /**
   * Ignore the website sitemap when crawling.
   * @type {boolean}
   * @memberof MapUrlsRequest
   */
  ignoreSitemap?: boolean;
  /**
   * Only return links found in the website sitemap
   * @type {boolean}
   * @memberof MapUrlsRequest
   */
  sitemapOnly?: boolean;
  /**
   * Include subdomains of the website
   * @type {boolean}
   * @memberof MapUrlsRequest
   */
  includeSubdomains?: boolean;
  /**
   * Maximum number of links to return
   * @type {number}
   * @memberof MapUrlsRequest
   */
  limit?: number;
}
/**
 * Press a key on the page. See https://asawicki.info/nosense/doc/devices/keyboard/key_codes.html for key codes.
 * @export
 * @interface PressAKey
 */
export interface PressAKey {
  /**
   * Press a key on the page
   * @type {string}
   * @memberof PressAKey
   */
  type: PressAKeyTypeEnum;
  /**
   * Key to press
   * @type {string}
   * @memberof PressAKey
   */
  key: string;
}

export const PressAKeyTypeEnum = {
  Press: 'press',
} as const;

export type PressAKeyTypeEnum = (typeof PressAKeyTypeEnum)[keyof typeof PressAKeyTypeEnum];

/**
 *
 * @export
 * @interface Scrape
 */
export interface Scrape {
  /**
   * Scrape the current page content, returns the url and the html.
   * @type {string}
   * @memberof Scrape
   */
  type: ScrapeTypeEnum;
}

export const ScrapeTypeEnum = {
  Scrape: 'scrape',
} as const;

export type ScrapeTypeEnum = (typeof ScrapeTypeEnum)[keyof typeof ScrapeTypeEnum];

/**
 *
 * @export
 * @interface ScrapeAndExtractFromUrl402Response
 */
export interface ScrapeAndExtractFromUrl402Response {
  /**
   *
   * @type {string}
   * @memberof ScrapeAndExtractFromUrl402Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface ScrapeAndExtractFromUrl429Response
 */
export interface ScrapeAndExtractFromUrl429Response {
  /**
   *
   * @type {string}
   * @memberof ScrapeAndExtractFromUrl429Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface ScrapeAndExtractFromUrl500Response
 */
export interface ScrapeAndExtractFromUrl500Response {
  /**
   *
   * @type {string}
   * @memberof ScrapeAndExtractFromUrl500Response
   */
  error?: string;
}
/**
 *
 * @export
 * @interface ScrapeAndExtractFromUrlRequest
 */
export interface ScrapeAndExtractFromUrlRequest {
  /**
   * The URL to scrape
   * @type {string}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  url: string;
  /**
   * Formats to include in the output.
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  formats?: Array<ScrapeAndExtractFromUrlRequestFormatsEnum>;
  /**
   * Only return the main content of the page excluding headers, navs, footers, etc.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  onlyMainContent?: boolean;
  /**
   * Tags to include in the output.
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  includeTags?: Array<string>;
  /**
   * Tags to exclude from the output.
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  excludeTags?: Array<string>;
  /**
   * Headers to send with the request. Can be used to send cookies, user-agent, etc.
   * @type {object}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  headers?: object;
  /**
   * Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.
   * @type {number}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  waitFor?: number;
  /**
   * Set to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  mobile?: boolean;
  /**
   * Skip TLS certificate verification when making requests
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  skipTlsVerification?: boolean;
  /**
   * Timeout in milliseconds for the request
   * @type {number}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  timeout?: number;
  /**
   *
   * @type {ScrapeAndExtractFromUrlRequestExtract}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  extract?: ScrapeAndExtractFromUrlRequestExtract;
  /**
   * Actions to perform on the page before grabbing the content
   * @type {Array<ScrapeAndExtractFromUrlRequestActionsInner>}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  actions?: Array<ScrapeAndExtractFromUrlRequestActionsInner>;
  /**
   *
   * @type {ScrapeAndExtractFromUrlRequestLocation}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  location?: ScrapeAndExtractFromUrlRequestLocation;
  /**
   * Removes all base 64 images from the output, which may be overwhelmingly long. The image\'s alt text remains in the output, but the URL is replaced with a placeholder.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlRequest
   */
  removeBase64Images?: boolean;
}

export const ScrapeAndExtractFromUrlRequestFormatsEnum = {
  Markdown: 'markdown',
  Html: 'html',
  RawHtml: 'rawHtml',
  Links: 'links',
  Screenshot: 'screenshot',
  Extract: 'extract',
  ScreenshotfullPage: 'screenshot@fullPage',
} as const;

export type ScrapeAndExtractFromUrlRequestFormatsEnum =
  (typeof ScrapeAndExtractFromUrlRequestFormatsEnum)[keyof typeof ScrapeAndExtractFromUrlRequestFormatsEnum];

/**
 * @type ScrapeAndExtractFromUrlRequestActionsInner
 * @export
 */
export type ScrapeAndExtractFromUrlRequestActionsInner =
  | Click
  | ExecuteJavaScript
  | PressAKey
  | Scrape
  | Screenshot
  | Scroll
  | Wait
  | WriteText;

/**
 * Extract object
 * @export
 * @interface ScrapeAndExtractFromUrlRequestExtract
 */
export interface ScrapeAndExtractFromUrlRequestExtract {
  /**
   * The schema to use for the extraction (Optional)
   * @type {object}
   * @memberof ScrapeAndExtractFromUrlRequestExtract
   */
  schema?: object;
  /**
   * The system prompt to use for the extraction (Optional)
   * @type {string}
   * @memberof ScrapeAndExtractFromUrlRequestExtract
   */
  systemPrompt?: string;
  /**
   * The prompt to use for the extraction without a schema (Optional)
   * @type {string}
   * @memberof ScrapeAndExtractFromUrlRequestExtract
   */
  prompt?: string;
}
/**
 * Location settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to \'US\' if not specified.
 * @export
 * @interface ScrapeAndExtractFromUrlRequestLocation
 */
export interface ScrapeAndExtractFromUrlRequestLocation {
  /**
   * ISO 3166-1 alpha-2 country code (e.g., \'US\', \'AU\', \'DE\', \'JP\')
   * @type {string}
   * @memberof ScrapeAndExtractFromUrlRequestLocation
   */
  country?: string;
  /**
   * Preferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlRequestLocation
   */
  languages?: Array<string>;
}
/**
 *
 * @export
 * @interface ScrapeAndExtractFromUrlsRequest
 */
export interface ScrapeAndExtractFromUrlsRequest {
  /**
   *
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  urls?: Array<string>;
  /**
   *
   * @type {ScrapeAndExtractFromUrlsRequestWebhook}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  webhook?: ScrapeAndExtractFromUrlsRequestWebhook;
  /**
   * Formats to include in the output.
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  formats?: Array<ScrapeAndExtractFromUrlsRequestFormatsEnum>;
  /**
   * Only return the main content of the page excluding headers, navs, footers, etc.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  onlyMainContent?: boolean;
  /**
   * Tags to include in the output.
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  includeTags?: Array<string>;
  /**
   * Tags to exclude from the output.
   * @type {Array<string>}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  excludeTags?: Array<string>;
  /**
   * Headers to send with the request. Can be used to send cookies, user-agent, etc.
   * @type {object}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  headers?: object;
  /**
   * Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.
   * @type {number}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  waitFor?: number;
  /**
   * Set to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  mobile?: boolean;
  /**
   * Skip TLS certificate verification when making requests
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  skipTlsVerification?: boolean;
  /**
   * Timeout in milliseconds for the request
   * @type {number}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  timeout?: number;
  /**
   *
   * @type {ScrapeAndExtractFromUrlRequestExtract}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  extract?: ScrapeAndExtractFromUrlRequestExtract;
  /**
   * Actions to perform on the page before grabbing the content
   * @type {Array<ScrapeAndExtractFromUrlsRequestActionsInner>}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  actions?: Array<ScrapeAndExtractFromUrlsRequestActionsInner>;
  /**
   *
   * @type {ScrapeAndExtractFromUrlRequestLocation}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  location?: ScrapeAndExtractFromUrlRequestLocation;
  /**
   * Removes all base 64 images from the output, which may be overwhelmingly long. The image\'s alt text remains in the output, but the URL is replaced with a placeholder.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  removeBase64Images?: boolean;
  /**
   * If invalid URLs are specified in the urls array, they will be ignored. Instead of them failing the entire request, a batch scrape using the remaining valid URLs will be created, and the invalid URLs will be returned in the invalidURLs field of the response.
   * @type {boolean}
   * @memberof ScrapeAndExtractFromUrlsRequest
   */
  ignoreInvalidURLs?: boolean;
}

export const ScrapeAndExtractFromUrlsRequestFormatsEnum = {
  Markdown: 'markdown',
  Html: 'html',
  RawHtml: 'rawHtml',
  Links: 'links',
  Screenshot: 'screenshot',
  Extract: 'extract',
  ScreenshotfullPage: 'screenshot@fullPage',
} as const;

export type ScrapeAndExtractFromUrlsRequestFormatsEnum =
  (typeof ScrapeAndExtractFromUrlsRequestFormatsEnum)[keyof typeof ScrapeAndExtractFromUrlsRequestFormatsEnum];

/**
 * @type ScrapeAndExtractFromUrlsRequestActionsInner
 * @export
 */
export type ScrapeAndExtractFromUrlsRequestActionsInner =
  | Click
  | ExecuteJavaScript
  | PressAKey
  | Scrape
  | Screenshot
  | Scroll
  | Wait
  | WriteText;

/**
 * @type ScrapeAndExtractFromUrlsRequestWebhook
 * @export
 */
export type ScrapeAndExtractFromUrlsRequestWebhook =
  | ScrapeAndExtractFromUrlsRequestWebhookOneOf
  | string;

/**
 * A complex webhook specification object.
 * @export
 * @interface ScrapeAndExtractFromUrlsRequestWebhookOneOf
 */
export interface ScrapeAndExtractFromUrlsRequestWebhookOneOf {
  /**
   * The URL to send the webhook to. This will trigger for batch scrape started (batch_scrape.started), every page scraped (batch_scrape.page) and when the batch scrape is completed (batch_scrape.completed or batch_scrape.failed). The response will be the same as the `/scrape` endpoint.
   * @type {string}
   * @memberof ScrapeAndExtractFromUrlsRequestWebhookOneOf
   */
  url: string;
  /**
   * Headers to send to the webhook URL.
   * @type {{ [key: string]: string; }}
   * @memberof ScrapeAndExtractFromUrlsRequestWebhookOneOf
   */
  headers?: { [key: string]: string };
  /**
   * Custom metadata that will be included in all webhook payloads for this crawl
   * @type {{ [key: string]: any; }}
   * @memberof ScrapeAndExtractFromUrlsRequestWebhookOneOf
   */
  metadata?: { [key: string]: any };
}
/**
 *
 * @export
 * @interface ScrapeResponse
 */
export interface ScrapeResponse {
  /**
   *
   * @type {boolean}
   * @memberof ScrapeResponse
   */
  success?: boolean;
  /**
   *
   * @type {ScrapeResponseData}
   * @memberof ScrapeResponse
   */
  data?: ScrapeResponseData;
}
/**
 *
 * @export
 * @interface ScrapeResponseData
 */
export interface ScrapeResponseData {
  /**
   *
   * @type {string}
   * @memberof ScrapeResponseData
   */
  markdown?: string;
  /**
   * HTML version of the content on page if `html` is in `formats`
   * @type {string}
   * @memberof ScrapeResponseData
   */
  html?: string | null;
  /**
   * Raw HTML content of the page if `rawHtml` is in `formats`
   * @type {string}
   * @memberof ScrapeResponseData
   */
  rawHtml?: string | null;
  /**
   * Screenshot of the page if `screenshot` is in `formats`
   * @type {string}
   * @memberof ScrapeResponseData
   */
  screenshot?: string | null;
  /**
   * List of links on the page if `links` is in `formats`
   * @type {Array<string>}
   * @memberof ScrapeResponseData
   */
  links?: Array<string>;
  /**
   *
   * @type {ScrapeResponseDataActions}
   * @memberof ScrapeResponseData
   */
  actions?: ScrapeResponseDataActions | null;
  /**
   *
   * @type {ScrapeResponseDataMetadata}
   * @memberof ScrapeResponseData
   */
  metadata?: ScrapeResponseDataMetadata;
  /**
   * Displayed when using LLM Extraction. Extracted data from the page following the schema defined.
   * @type {object}
   * @memberof ScrapeResponseData
   */
  llm_extraction?: object | null;
  /**
   * Can be displayed when using LLM Extraction. Warning message will let you know any issues with the extraction.
   * @type {string}
   * @memberof ScrapeResponseData
   */
  warning?: string | null;
}
/**
 * Results of the actions specified in the `actions` parameter. Only present if the `actions` parameter was provided in the request
 * @export
 * @interface ScrapeResponseDataActions
 */
export interface ScrapeResponseDataActions {
  /**
   * Screenshot URLs, in the same order as the screenshot actions provided.
   * @type {Array<string>}
   * @memberof ScrapeResponseDataActions
   */
  screenshots?: Array<string>;
}
/**
 *
 * @export
 * @interface ScrapeResponseDataMetadata
 */
export interface ScrapeResponseDataMetadata {
  /**
   *
   * @type {string}
   * @memberof ScrapeResponseDataMetadata
   */
  title?: string;
  /**
   *
   * @type {string}
   * @memberof ScrapeResponseDataMetadata
   */
  description?: string;
  /**
   *
   * @type {string}
   * @memberof ScrapeResponseDataMetadata
   */
  language?: string | null;
  /**
   *
   * @type {string}
   * @memberof ScrapeResponseDataMetadata
   */
  sourceURL?: string;
  /**
   *
   * @type {string}
   * @memberof ScrapeResponseDataMetadata
   */
  '&lt;any other metadata&gt; '?: string;
  /**
   * The status code of the page
   * @type {number}
   * @memberof ScrapeResponseDataMetadata
   */
  statusCode?: number;
  /**
   * The error message of the page
   * @type {string}
   * @memberof ScrapeResponseDataMetadata
   */
  error?: string | null;
}
/**
 *
 * @export
 * @interface Screenshot
 */
export interface Screenshot {
  /**
   * Take a screenshot
   * @type {string}
   * @memberof Screenshot
   */
  type: ScreenshotTypeEnum;
  /**
   * Should the screenshot be full-page or viewport sized?
   * @type {boolean}
   * @memberof Screenshot
   */
  fullPage?: boolean;
}

export const ScreenshotTypeEnum = {
  Screenshot: 'screenshot',
} as const;

export type ScreenshotTypeEnum = (typeof ScreenshotTypeEnum)[keyof typeof ScreenshotTypeEnum];

/**
 *
 * @export
 * @interface Scroll
 */
export interface Scroll {
  /**
   * Scroll the page or a specific element
   * @type {string}
   * @memberof Scroll
   */
  type: ScrollTypeEnum;
  /**
   * Direction to scroll
   * @type {string}
   * @memberof Scroll
   */
  direction?: ScrollDirectionEnum;
  /**
   * Query selector for the element to scroll
   * @type {string}
   * @memberof Scroll
   */
  selector?: string;
}

export const ScrollTypeEnum = {
  Scroll: 'scroll',
} as const;

export type ScrollTypeEnum = (typeof ScrollTypeEnum)[keyof typeof ScrollTypeEnum];
export const ScrollDirectionEnum = {
  Up: 'up',
  Down: 'down',
} as const;

export type ScrollDirectionEnum = (typeof ScrollDirectionEnum)[keyof typeof ScrollDirectionEnum];

/**
 *
 * @export
 * @interface Wait
 */
export interface Wait {
  /**
   * Wait for a specified amount of milliseconds
   * @type {string}
   * @memberof Wait
   */
  type: WaitTypeEnum;
  /**
   * Number of milliseconds to wait
   * @type {number}
   * @memberof Wait
   */
  milliseconds?: number;
  /**
   * Query selector to find the element by
   * @type {string}
   * @memberof Wait
   */
  selector?: string;
}

export const WaitTypeEnum = {
  Wait: 'wait',
} as const;

export type WaitTypeEnum = (typeof WaitTypeEnum)[keyof typeof WaitTypeEnum];

/**
 *
 * @export
 * @interface WriteText
 */
export interface WriteText {
  /**
   * Write text into an input field, text area, or contenteditable element. Note: You must first focus the element using a \'click\' action before writing. The text will be typed character by character to simulate keyboard input.
   * @type {string}
   * @memberof WriteText
   */
  type: WriteTextTypeEnum;
  /**
   * Text to type
   * @type {string}
   * @memberof WriteText
   */
  text: string;
}

export const WriteTextTypeEnum = {
  Write: 'write',
} as const;

export type WriteTextTypeEnum = (typeof WriteTextTypeEnum)[keyof typeof WriteTextTypeEnum];

/**
 * BillingApi - axios parameter creator
 * @export
 */
export const BillingApiAxiosParamCreator = function (configuration?: Configuration) {
  return {
    /**
     *
     * @summary Get remaining credits for the authenticated team
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    getCreditUsage: async (options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
      const localVarPath = `/team/credit-usage`;
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'GET',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
  };
};

/**
 * BillingApi - functional programming interface
 * @export
 */
export const BillingApiFp = function (configuration?: Configuration) {
  const localVarAxiosParamCreator = BillingApiAxiosParamCreator(configuration);
  return {
    /**
     *
     * @summary Get remaining credits for the authenticated team
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async getCreditUsage(
      options?: RawAxiosRequestConfig,
    ): Promise<
      (axios?: AxiosInstance, basePath?: string) => AxiosPromise<GetCreditUsage200Response>
    > {
      const localVarAxiosArgs = await localVarAxiosParamCreator.getCreditUsage(options);
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['BillingApi.getCreditUsage']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
  };
};

/**
 * BillingApi - factory interface
 * @export
 */
export const BillingApiFactory = function (
  configuration?: Configuration,
  basePath?: string,
  axios?: AxiosInstance,
) {
  const localVarFp = BillingApiFp(configuration);
  return {
    /**
     *
     * @summary Get remaining credits for the authenticated team
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    getCreditUsage(options?: RawAxiosRequestConfig): AxiosPromise<GetCreditUsage200Response> {
      return localVarFp.getCreditUsage(options).then((request) => request(axios, basePath));
    },
  };
};

/**
 * BillingApi - object-oriented interface
 * @export
 * @class BillingApi
 * @extends {BaseAPI}
 */
export class BillingApi extends BaseAPI {
  /**
   *
   * @summary Get remaining credits for the authenticated team
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof BillingApi
   */
  public getCreditUsage(options?: RawAxiosRequestConfig) {
    return BillingApiFp(this.configuration)
      .getCreditUsage(options)
      .then((request) => request(this.axios, this.basePath));
  }
}

/**
 * CrawlingApi - axios parameter creator
 * @export
 */
export const CrawlingApiAxiosParamCreator = function (configuration?: Configuration) {
  return {
    /**
     *
     * @summary Cancel a crawl job
     * @param {string} id The ID of the batch scrape job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    cancelBatchCrawl: async (
      id: string,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'id' is not null or undefined
      assertParamExists('cancelBatchCrawl', 'id', id);
      const localVarPath = `/batch/scrape/{id}`.replace(
        `{${'id'}}`,
        encodeURIComponent(String(id)),
      );
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'DELETE',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
    /**
     *
     * @summary Cancel a crawl job
     * @param {string} id The ID of the crawl job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    cancelCrawl: async (id: string, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
      // verify required parameter 'id' is not null or undefined
      assertParamExists('cancelCrawl', 'id', id);
      const localVarPath = `/crawl/{id}`.replace(`{${'id'}}`, encodeURIComponent(String(id)));
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'DELETE',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
    /**
     *
     * @summary Crawl multiple URLs based on options
     * @param {CrawlUrlsRequest} crawlUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    crawlUrls: async (
      crawlUrlsRequest: CrawlUrlsRequest,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'crawlUrlsRequest' is not null or undefined
      assertParamExists('crawlUrls', 'crawlUrlsRequest', crawlUrlsRequest);
      const localVarPath = `/crawl`;
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'POST',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      localVarHeaderParameter['Content-Type'] = 'application/json';

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };
      localVarRequestOptions.data = serializeDataIfNeeded(
        crawlUrlsRequest,
        localVarRequestOptions,
        configuration,
      );

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
    /**
     *
     * @summary Get the status of a crawl job
     * @param {string} id The ID of the crawl job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    getCrawlStatus: async (
      id: string,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'id' is not null or undefined
      assertParamExists('getCrawlStatus', 'id', id);
      const localVarPath = `/crawl/{id}`.replace(`{${'id'}}`, encodeURIComponent(String(id)));
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'GET',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
  };
};

/**
 * CrawlingApi - functional programming interface
 * @export
 */
export const CrawlingApiFp = function (configuration?: Configuration) {
  const localVarAxiosParamCreator = CrawlingApiAxiosParamCreator(configuration);
  return {
    /**
     *
     * @summary Cancel a crawl job
     * @param {string} id The ID of the batch scrape job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async cancelBatchCrawl(
      id: string,
      options?: RawAxiosRequestConfig,
    ): Promise<
      (axios?: AxiosInstance, basePath?: string) => AxiosPromise<CancelBatchCrawl200Response>
    > {
      const localVarAxiosArgs = await localVarAxiosParamCreator.cancelBatchCrawl(id, options);
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['CrawlingApi.cancelBatchCrawl']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
    /**
     *
     * @summary Cancel a crawl job
     * @param {string} id The ID of the crawl job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async cancelCrawl(
      id: string,
      options?: RawAxiosRequestConfig,
    ): Promise<
      (axios?: AxiosInstance, basePath?: string) => AxiosPromise<CancelBatchCrawl200Response>
    > {
      const localVarAxiosArgs = await localVarAxiosParamCreator.cancelCrawl(id, options);
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['CrawlingApi.cancelCrawl']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
    /**
     *
     * @summary Crawl multiple URLs based on options
     * @param {CrawlUrlsRequest} crawlUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async crawlUrls(
      crawlUrlsRequest: CrawlUrlsRequest,
      options?: RawAxiosRequestConfig,
    ): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<CrawlResponse>> {
      const localVarAxiosArgs = await localVarAxiosParamCreator.crawlUrls(
        crawlUrlsRequest,
        options,
      );
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['CrawlingApi.crawlUrls']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
    /**
     *
     * @summary Get the status of a crawl job
     * @param {string} id The ID of the crawl job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async getCrawlStatus(
      id: string,
      options?: RawAxiosRequestConfig,
    ): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<CrawlStatusResponseObj>> {
      const localVarAxiosArgs = await localVarAxiosParamCreator.getCrawlStatus(id, options);
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['CrawlingApi.getCrawlStatus']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
  };
};

/**
 * CrawlingApi - factory interface
 * @export
 */
export const CrawlingApiFactory = function (
  configuration?: Configuration,
  basePath?: string,
  axios?: AxiosInstance,
) {
  const localVarFp = CrawlingApiFp(configuration);
  return {
    /**
     *
     * @summary Cancel a crawl job
     * @param {string} id The ID of the batch scrape job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    cancelBatchCrawl(
      id: string,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<CancelBatchCrawl200Response> {
      return localVarFp.cancelBatchCrawl(id, options).then((request) => request(axios, basePath));
    },
    /**
     *
     * @summary Cancel a crawl job
     * @param {string} id The ID of the crawl job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    cancelCrawl(
      id: string,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<CancelBatchCrawl200Response> {
      return localVarFp.cancelCrawl(id, options).then((request) => request(axios, basePath));
    },
    /**
     *
     * @summary Crawl multiple URLs based on options
     * @param {CrawlUrlsRequest} crawlUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    crawlUrls(
      crawlUrlsRequest: CrawlUrlsRequest,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<CrawlResponse> {
      return localVarFp
        .crawlUrls(crawlUrlsRequest, options)
        .then((request) => request(axios, basePath));
    },
    /**
     *
     * @summary Get the status of a crawl job
     * @param {string} id The ID of the crawl job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    getCrawlStatus(
      id: string,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<CrawlStatusResponseObj> {
      return localVarFp.getCrawlStatus(id, options).then((request) => request(axios, basePath));
    },
  };
};

/**
 * CrawlingApi - object-oriented interface
 * @export
 * @class CrawlingApi
 * @extends {BaseAPI}
 */
/**
 * @export
 * @interface CrawlingApi
 */
export interface CrawlingApi {
  /**
   * Cancel a batch crawl job.
   * @param id The ID of the batch crawl job.
   * @param options The request options.
   */
  cancelBatchCrawl(
    id: string,
    options?: RawAxiosRequestConfig,
  ): AxiosPromise<CancelBatchCrawl200Response>;
  /**
   * Cancel a crawl job.
   * @param id The ID of the crawl job.
   * @param options The request options.
   */
  cancelCrawl(
    id: string,
    options?: RawAxiosRequestConfig,
  ): AxiosPromise<CancelBatchCrawl200Response>;
  /**
   * Start a new crawl job.
   * @param crawlUrlsRequest The crawl request payload.
   * @param options The request options.
   */
  crawlUrls(
    crawlUrlsRequest: CrawlUrlsRequest,
    options?: RawAxiosRequestConfig,
  ): AxiosPromise<CrawlResponse>;
  /**
   * Get the status of a crawl job.
   * @param id The ID of the crawl job.
   * @param options The request options.
   */
  getCrawlStatus(id: string, options?: RawAxiosRequestConfig): AxiosPromise<CrawlStatusResponseObj>;
  /**
   * Get the files generated by a completed crawl job.
   * @param id The ID of the crawl job.
   * @param options The request options.
   */
  getCrawlFiles(id: string, options?: RawAxiosRequestConfig): AxiosPromise<any>;
}

/**
 * CrawlingApi - object-oriented interface for crawl operations.
 * Provides methods to cancel, start, and get the status of crawl jobs.
 * @export
 * @class CrawlingApi
 * @extends {BaseAPI}
 */
export class CrawlingApi extends BaseAPI {
  /**
   * Creates a new instance of CrawlingApi.
   * @param configuration Optional configuration object.
   * @param basePath Optional base path.
   * @param axios Optional Axios instance.
   */
  constructor(configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    super(configuration, basePath, axios);
  }

  /**
   * Cancel a batch crawl job.
   * @param {string} id The ID of the batch scrape job
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof CrawlingApi
   */
  public cancelBatchCrawl(id: string, options?: RawAxiosRequestConfig) {
    return CrawlingApiFp(this.configuration)
      .cancelBatchCrawl(id, options)
      .then((request) => request(this.axios, this.basePath));
  }

  /**
   * Cancel a crawl job.
   * @param {string} id The ID of the crawl job
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof CrawlingApi
   */
  public cancelCrawl(id: string, options?: RawAxiosRequestConfig) {
    return CrawlingApiFp(this.configuration)
      .cancelCrawl(id, options)
      .then((request) => request(this.axios, this.basePath));
  }

  /**
   * Start a new crawl job.
   * @param {CrawlUrlsRequest} crawlUrlsRequest
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof CrawlingApi
   */
  public crawlUrls(crawlUrlsRequest: CrawlUrlsRequest, options?: RawAxiosRequestConfig) {
    return CrawlingApiFp(this.configuration)
      .crawlUrls(crawlUrlsRequest, options)
      .then((request) => request(this.axios, this.basePath));
  }

  /**
   * Get the status of a crawl job.
   * @param {string} id The ID of the crawl job
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof CrawlingApi
   */
  public getCrawlStatus(id: string, options?: RawAxiosRequestConfig) {
    return CrawlingApiFp(this.configuration)
      .getCrawlStatus(id, options)
      .then((request) => request(this.axios, this.basePath));
  }
}

/**
 * ExtractionApi - axios parameter creator
 * @export
 */
export const ExtractionApiAxiosParamCreator = function (configuration?: Configuration) {
  return {
    /**
     *
     * @summary Extract structured data from pages using LLMs
     * @param {ExtractDataRequest} extractDataRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    extractData: async (
      extractDataRequest: ExtractDataRequest,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'extractDataRequest' is not null or undefined
      assertParamExists('extractData', 'extractDataRequest', extractDataRequest);
      const localVarPath = `/extract`;
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'POST',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      localVarHeaderParameter['Content-Type'] = 'application/json';

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };
      localVarRequestOptions.data = serializeDataIfNeeded(
        extractDataRequest,
        localVarRequestOptions,
        configuration,
      );

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
  };
};

/**
 * ExtractionApi - functional programming interface
 * @export
 */
export const ExtractionApiFp = function (configuration?: Configuration) {
  const localVarAxiosParamCreator = ExtractionApiAxiosParamCreator(configuration);
  return {
    /**
     *
     * @summary Extract structured data from pages using LLMs
     * @param {ExtractDataRequest} extractDataRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async extractData(
      extractDataRequest: ExtractDataRequest,
      options?: RawAxiosRequestConfig,
    ): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<ExtractResponse>> {
      const localVarAxiosArgs = await localVarAxiosParamCreator.extractData(
        extractDataRequest,
        options,
      );
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['ExtractionApi.extractData']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
  };
};

/**
 * ExtractionApi - factory interface
 * @export
 */
export const ExtractionApiFactory = function (
  configuration?: Configuration,
  basePath?: string,
  axios?: AxiosInstance,
) {
  const localVarFp = ExtractionApiFp(configuration);
  return {
    /**
     *
     * @summary Extract structured data from pages using LLMs
     * @param {ExtractDataRequest} extractDataRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    extractData(
      extractDataRequest: ExtractDataRequest,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<ExtractResponse> {
      return localVarFp
        .extractData(extractDataRequest, options)
        .then((request) => request(axios, basePath));
    },
  };
};

/**
 * ExtractionApi - object-oriented interface
 * @export
 * @class ExtractionApi
 * @extends {BaseAPI}
 */
/**
 * ExtractionApi - object-oriented interface for extraction operations.
 * Provides methods to extract structured data from pages using LLMs.
 * @export
 * @class ExtractionApi
 * @extends {BaseAPI}
 */
export class ExtractionApi extends BaseAPI {
  /**
   * Creates a new instance of ExtractionApi.
   * @param configuration Optional configuration object.
   * @param basePath Optional base path.
   * @param axios Optional Axios instance.
   */
  constructor(configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    super(configuration, basePath, axios);
  }

  /**
   * Extract structured data from pages using LLMs.
   * @param {ExtractDataRequest} extractDataRequest
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof ExtractionApi
   */
  public extractData(extractDataRequest: ExtractDataRequest, options?: RawAxiosRequestConfig) {
    return ExtractionApiFp(this.configuration)
      .extractData(extractDataRequest, options)
      .then((request) => request(this.axios, this.basePath));
  }
}

/**
 * MappingApi - axios parameter creator
 * @export
 */
export const MappingApiAxiosParamCreator = function (configuration?: Configuration) {
  return {
    /**
     *
     * @summary Map multiple URLs based on options
     * @param {MapUrlsRequest} mapUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    mapUrls: async (
      mapUrlsRequest: MapUrlsRequest,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'mapUrlsRequest' is not null or undefined
      assertParamExists('mapUrls', 'mapUrlsRequest', mapUrlsRequest);
      const localVarPath = `/map`;
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'POST',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      localVarHeaderParameter['Content-Type'] = 'application/json';

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };
      localVarRequestOptions.data = serializeDataIfNeeded(
        mapUrlsRequest,
        localVarRequestOptions,
        configuration,
      );

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
  };
};

/**
 * MappingApi - functional programming interface
 * @export
 */
export const MappingApiFp = function (configuration?: Configuration) {
  const localVarAxiosParamCreator = MappingApiAxiosParamCreator(configuration);
  return {
    /**
     *
     * @summary Map multiple URLs based on options
     * @param {MapUrlsRequest} mapUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async mapUrls(
      mapUrlsRequest: MapUrlsRequest,
      options?: RawAxiosRequestConfig,
    ): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<MapResponse>> {
      const localVarAxiosArgs = await localVarAxiosParamCreator.mapUrls(mapUrlsRequest, options);
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['MappingApi.mapUrls']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
  };
};

/**
 * MappingApi - factory interface
 * @export
 */
export const MappingApiFactory = function (
  configuration?: Configuration,
  basePath?: string,
  axios?: AxiosInstance,
) {
  const localVarFp = MappingApiFp(configuration);
  return {
    /**
     *
     * @summary Map multiple URLs based on options
     * @param {MapUrlsRequest} mapUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    mapUrls(
      mapUrlsRequest: MapUrlsRequest,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<MapResponse> {
      return localVarFp
        .mapUrls(mapUrlsRequest, options)
        .then((request) => request(axios, basePath));
    },
  };
};

/**
 * MappingApi - object-oriented interface
 * @export
 * @class MappingApi
 * @extends {BaseAPI}
 */
/**
 * MappingApi - object-oriented interface for mapping operations.
 * Provides methods to map multiple URLs based on options.
 * @export
 * @class MappingApi
 * @extends {BaseAPI}
 */
export class MappingApi extends BaseAPI {
  /**
   * Creates a new instance of MappingApi.
   * @param configuration Optional configuration object.
   * @param basePath Optional base path.
   * @param axios Optional Axios instance.
   */
  constructor(configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    super(configuration, basePath, axios);
  }

  /**
   * Map multiple URLs based on options.
   * @param {MapUrlsRequest} mapUrlsRequest
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof MappingApi
   */
  public mapUrls(mapUrlsRequest: MapUrlsRequest, options?: RawAxiosRequestConfig) {
    return MappingApiFp(this.configuration)
      .mapUrls(mapUrlsRequest, options)
      .then((request) => request(this.axios, this.basePath));
  }
}

/**
 * ScrapingApi - axios parameter creator
 * @export
 */
export const ScrapingApiAxiosParamCreator = function (configuration?: Configuration) {
  return {
    /**
     *
     * @summary Get the status of a batch scrape job
     * @param {string} id The ID of the batch scrape job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    getBatchScrapeStatus: async (
      id: string,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'id' is not null or undefined
      assertParamExists('getBatchScrapeStatus', 'id', id);
      const localVarPath = `/batch/scrape/{id}`.replace(
        `{${'id'}}`,
        encodeURIComponent(String(id)),
      );
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'GET',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
    /**
     *
     * @summary Scrape a single URL and optionally extract information using an LLM
     * @param {ScrapeAndExtractFromUrlRequest} scrapeAndExtractFromUrlRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    scrapeAndExtractFromUrl: async (
      scrapeAndExtractFromUrlRequest: ScrapeAndExtractFromUrlRequest,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'scrapeAndExtractFromUrlRequest' is not null or undefined
      assertParamExists(
        'scrapeAndExtractFromUrl',
        'scrapeAndExtractFromUrlRequest',
        scrapeAndExtractFromUrlRequest,
      );
      const localVarPath = `/scrape`;
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'POST',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      localVarHeaderParameter['Content-Type'] = 'application/json';

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };
      localVarRequestOptions.data = serializeDataIfNeeded(
        scrapeAndExtractFromUrlRequest,
        localVarRequestOptions,
        configuration,
      );

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
    /**
     *
     * @summary Scrape multiple URLs and optionally extract information using an LLM
     * @param {ScrapeAndExtractFromUrlsRequest} scrapeAndExtractFromUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    scrapeAndExtractFromUrls: async (
      scrapeAndExtractFromUrlsRequest: ScrapeAndExtractFromUrlsRequest,
      options: RawAxiosRequestConfig = {},
    ): Promise<RequestArgs> => {
      // verify required parameter 'scrapeAndExtractFromUrlsRequest' is not null or undefined
      assertParamExists(
        'scrapeAndExtractFromUrls',
        'scrapeAndExtractFromUrlsRequest',
        scrapeAndExtractFromUrlsRequest,
      );
      const localVarPath = `/batch/scrape`;
      // use dummy base URL string because the URL constructor only accepts absolute URLs.
      const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
      let baseOptions;
      if (configuration) {
        baseOptions = configuration.baseOptions;
      }

      const localVarRequestOptions = {
        method: 'POST',
        ...baseOptions,
        ...options,
      };
      const localVarHeaderParameter = {} as any;
      const localVarQueryParameter = {} as any;

      localVarHeaderParameter['Content-Type'] = 'application/json';

      setSearchParams(localVarUrlObj, localVarQueryParameter);
      let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
      localVarRequestOptions.headers = {
        ...localVarHeaderParameter,
        ...headersFromBaseOptions,
        ...options.headers,
      };
      localVarRequestOptions.data = serializeDataIfNeeded(
        scrapeAndExtractFromUrlsRequest,
        localVarRequestOptions,
        configuration,
      );

      return {
        url: toPathString(localVarUrlObj),
        options: localVarRequestOptions,
      };
    },
  };
};

/**
 * ScrapingApi - functional programming interface
 * @export
 */
export const ScrapingApiFp = function (configuration?: Configuration) {
  const localVarAxiosParamCreator = ScrapingApiAxiosParamCreator(configuration);
  return {
    /**
     *
     * @summary Get the status of a batch scrape job
     * @param {string} id The ID of the batch scrape job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async getBatchScrapeStatus(
      id: string,
      options?: RawAxiosRequestConfig,
    ): Promise<
      (axios?: AxiosInstance, basePath?: string) => AxiosPromise<BatchScrapeStatusResponseObj>
    > {
      const localVarAxiosArgs = await localVarAxiosParamCreator.getBatchScrapeStatus(id, options);
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['ScrapingApi.getBatchScrapeStatus']?.[localVarOperationServerIndex]?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
    /**
     *
     * @summary Scrape a single URL and optionally extract information using an LLM
     * @param {ScrapeAndExtractFromUrlRequest} scrapeAndExtractFromUrlRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async scrapeAndExtractFromUrl(
      scrapeAndExtractFromUrlRequest: ScrapeAndExtractFromUrlRequest,
      options?: RawAxiosRequestConfig,
    ): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<ScrapeResponse>> {
      const localVarAxiosArgs = await localVarAxiosParamCreator.scrapeAndExtractFromUrl(
        scrapeAndExtractFromUrlRequest,
        options,
      );
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['ScrapingApi.scrapeAndExtractFromUrl']?.[localVarOperationServerIndex]
          ?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
    /**
     *
     * @summary Scrape multiple URLs and optionally extract information using an LLM
     * @param {ScrapeAndExtractFromUrlsRequest} scrapeAndExtractFromUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    async scrapeAndExtractFromUrls(
      scrapeAndExtractFromUrlsRequest: ScrapeAndExtractFromUrlsRequest,
      options?: RawAxiosRequestConfig,
    ): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<BatchScrapeResponseObj>> {
      const localVarAxiosArgs = await localVarAxiosParamCreator.scrapeAndExtractFromUrls(
        scrapeAndExtractFromUrlsRequest,
        options,
      );
      const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
      const localVarOperationServerBasePath =
        operationServerMap['ScrapingApi.scrapeAndExtractFromUrls']?.[localVarOperationServerIndex]
          ?.url;
      return (axios, basePath) =>
        createRequestFunction(
          localVarAxiosArgs,
          globalAxios,
          BASE_PATH,
          configuration,
        )(axios, localVarOperationServerBasePath || basePath);
    },
  };
};

/**
 * ScrapingApi - factory interface
 * @export
 */
export const ScrapingApiFactory = function (
  configuration?: Configuration,
  basePath?: string,
  axios?: AxiosInstance,
) {
  const localVarFp = ScrapingApiFp(configuration);
  return {
    /**
     *
     * @summary Get the status of a batch scrape job
     * @param {string} id The ID of the batch scrape job
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    getBatchScrapeStatus(
      id: string,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<BatchScrapeStatusResponseObj> {
      return localVarFp
        .getBatchScrapeStatus(id, options)
        .then((request) => request(axios, basePath));
    },
    /**
     *
     * @summary Scrape a single URL and optionally extract information using an LLM
     * @param {ScrapeAndExtractFromUrlRequest} scrapeAndExtractFromUrlRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    scrapeAndExtractFromUrl(
      scrapeAndExtractFromUrlRequest: ScrapeAndExtractFromUrlRequest,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<ScrapeResponse> {
      return localVarFp
        .scrapeAndExtractFromUrl(scrapeAndExtractFromUrlRequest, options)
        .then((request) => request(axios, basePath));
    },
    /**
     *
     * @summary Scrape multiple URLs and optionally extract information using an LLM
     * @param {ScrapeAndExtractFromUrlsRequest} scrapeAndExtractFromUrlsRequest
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     */
    scrapeAndExtractFromUrls(
      scrapeAndExtractFromUrlsRequest: ScrapeAndExtractFromUrlsRequest,
      options?: RawAxiosRequestConfig,
    ): AxiosPromise<BatchScrapeResponseObj> {
      return localVarFp
        .scrapeAndExtractFromUrls(scrapeAndExtractFromUrlsRequest, options)
        .then((request) => request(axios, basePath));
    },
  };
};

/**
 * ScrapingApi - object-oriented interface
 * @export
 * @class ScrapingApi
 * @extends {BaseAPI}
 */
/**
 * ScrapingApi - object-oriented interface for scraping operations.
 * Provides methods to scrape single or multiple URLs and extract information.
 * @export
 * @class ScrapingApi
 * @extends {BaseAPI}
 */
export class ScrapingApi extends BaseAPI {
  /**
   * Creates a new instance of ScrapingApi.
   * @param configuration Optional configuration object.
   * @param basePath Optional base path.
   * @param axios Optional Axios instance.
   */
  constructor(configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    super(configuration, basePath, axios);
  }

  /**
   * Get the status of a batch scrape job.
   * @param {string} id The ID of the batch scrape job
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof ScrapingApi
   */
  public getBatchScrapeStatus(id: string, options?: RawAxiosRequestConfig) {
    return ScrapingApiFp(this.configuration)
      .getBatchScrapeStatus(id, options)
      .then((request) => request(this.axios, this.basePath));
  }

  /**
   * Scrape a single URL and optionally extract information using an LLM.
   * @param {ScrapeAndExtractFromUrlRequest} scrapeAndExtractFromUrlRequest
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof ScrapingApi
   */
  public scrapeAndExtractFromUrl(
    scrapeAndExtractFromUrlRequest: ScrapeAndExtractFromUrlRequest,
    options?: RawAxiosRequestConfig,
  ) {
    return ScrapingApiFp(this.configuration)
      .scrapeAndExtractFromUrl(scrapeAndExtractFromUrlRequest, options)
      .then((request) => request(this.axios, this.basePath));
  }

  /**
   * Scrape multiple URLs and optionally extract information using an LLM.
   * @param {ScrapeAndExtractFromUrlsRequest} scrapeAndExtractFromUrlsRequest
   * @param {*} [options] Override http request option.
   * @throws {RequiredError}
   * @memberof ScrapingApi
   */
  public scrapeAndExtractFromUrls(
    scrapeAndExtractFromUrlsRequest: ScrapeAndExtractFromUrlsRequest,
    options?: RawAxiosRequestConfig,
  ) {
    return ScrapingApiFp(this.configuration)
      .scrapeAndExtractFromUrls(scrapeAndExtractFromUrlsRequest, options)
      .then((request) => request(this.axios, this.basePath));
  }
}
